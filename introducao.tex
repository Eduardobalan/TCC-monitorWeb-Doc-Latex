% Monitoramento de servidores Linux por web sites.
%====================================================================================================
% TCC
%----------------------------------------------------------------------------------------------------
% Autor				    : Eduardo Balan
% Orientador		  : Kleber Krugrer
% Instituição 		: UFMS - Universidade Federal do Mato Grosso do Sul
% Departamento		: CPCX - Sistema de Informação
%----------------------------------------------------------------------------------------------------
% Data de criação	: 29 de Março de 2017
%====================================================================================================

\pagenumbering{arabic}

\chapter{Introdução} \label{Cap:Introducao}

A Internet é uma rede de computadores que interconecta milhares de dispositivos computacionais ao redor do mundo com centenas de milhares de usuários. Há pouco tempo, esses dispositivos eram basicamente computadores de mesa e servidores que realizavam diversas tarefas, como armazenamento e distribuição de dados e arquivos, gerenciamento de impressão e de usuários, conexão a outras redes, transmissão de informações tais como páginas da \textit{web} e mensagens de \textit{e-mail}, além de outras funcionalidades \cite{Kurose:2010}. Com frequência, essas máquinas assim chamadas servidores são instaladas e mantidas em um local central de uma empresa por um administrador de sistemas \cite{Tanenbaum:2003}. 

Nos últimos tempos houve uma mudança, que é o uso da \textit{web} não apenas para comunicação, mas como uma forma de executar aplicativos.  Agora temos processadores de texto, planilhas e outros programas sendo executados como uma aplicação \textit{web} em um navegador, em que suas principais informações ficam armazenadas nos servidores \cite{Marimoto:2011}. Uma vez que os dados dos usuários estão armazenados em locais remotos algumas vantagens podem ser obtidas, tais como \textit{self-service} sob demanda e amplo acesso à rede, \cite{Sampaio:2003}  mas por outro lado, manter estes dados seguros é imprescindível e o acompanhamento destes servidores é uma forma de garantia. É fácil perceber que a queda de um servidor pode comprometer a produtividade de usuários, principalmente se esse servidor for o único disponível ou se estiver executando serviços vitais \cite{Weber:2002}.

Cabe observar, que segundo Marimoto, pouco a pouco, a internet tem se tornado o verdadeiro computador, e os computadores passam a ser cada vez mais um simples terminal, cuja única função é mostrar informações processadas por servidores remotos. Isso se tornou possível devido à popularização da ADSL (\textit{Assymetrical Digital Subscriber Line}), \textit{wireless} e outras formas de acesso rápido e contínuo à internet. Futuramente, a tendência é que mais aplicativos passem a ser usados via \textit{web}, tornando um computador desconectado cada vez mais limitado e inútil. Eventualmente, é possível que o próprio computador seja substituído por dispositivos mais simples e baratos, que sirvam como terminais de acesso \cite{Marimoto:2011}.


\section{Justificativa}

Muitas empresas detêm valiosas informações guardadas em seus servidores, que podem ser de ordem técnica (por exemplo, o projeto de um novo \textit{chip} ou novo \textit{software}), comercial (como estudos sobre competidores ou planos de \textit{marketing}), financeira (planos para uma venda de ações), jurídica (documentos sobre uma possível fusão ou aquisição), entre outras possibilidades. Além das ameaças causadas por invasores, dados valiosos podem ser perdidos por acidente. Algumas das causas comuns de perda acidental de dados são fenômenos naturais, como enchentes, terremotos, guerras, motins; erros de hardware ou de software (defeitos na CPU, discos ou fitas com problemas de leitura, surtos ou falhas de energia, sujeira,  erros de programas e temperaturas extremas); e erros humanos (entrada incorreta de dados, montagem incorreta de disco ou fita, execução de programas errado, entre outro) \cite{Tanenbaum:2010,Silberschatz:2000}.

Em 27 de dezembro de 2005, um incêndio destruiu seis dos dez andares do  prédio do INSS (Instituto Nacional de Seguro Social), em  Brasília \cite{Laudo:2006}. Segundo o ministro da Previdência e Assistência Social da época, Nelson Machado, as maiores perdas foram de informações de receita previdenciária, informações do sistema central e processos físicos, dívidas de empresas, processos de fraudes e autos de infração \cite{Machado:2005}. O presidente da Comissão de Fiscalização e Controle da Câmara dos Deputados, deputado Alexandre Cardoso, estimou, que os prejuízos, naquela época, referentes à processos administrativos foram equivalentes a R\$ 60 bilhões, tendo a previdência cópias de pelo menos R\$ 53 bilhões, concluindo que a união teria perdido em torno de R\$7 bilhões \cite{Futema:2005}.

A maioria dessas causas podem ser tratadas com a manutenção adequada dos \textit{backups}, preferivelmente em lugar distante dos dados originais. Embora proteger dados de perda acidental possa parecer banal, se comparado a proteger contra invasores inteligentes, na prática provavelmente mais danos são causados pelo primeiro que pelo ultimo \cite{Tanenbaum:2003, Silberschatz:2000}.


\section{Objetivos} \label{Sec:Objetivos}

\subsection{Objetivo Geral} \label{Sec:ObjetivoGeral}

O objetivo deste trabalho foi estudar tecnologias como Linux, C++, Java, Spring e PostgreSQL para criar um sistema de monitoramento para servidores Linux utilizando a arquitetura cliente-servidor. Outro objetivo foi realizar automaticamente procedimentos de \textit{backup} e \textit{vacuum} no banco de dados PostgreSQL. Esses procedimentos são de grande importância para identificar problemas, uma vez que esses servidores podem armazenar dados vitais, tais como dados financeiros, contas de usuário, relatórios corporativos, projetos de \textit{softwares} e \textit{hardware}, entre outros.


\subsection{Objetivos Específicos}\label{Sec:ObjetivosEspecificos}
\begin{itemize}
    \item Estudar um mecanismo que permitisse identificar as informações do \textit{hardware} (CPU, memória e \textit{swap}) em um sistema Linux;
    
    \item Estudar o banco de dados PostgreSQL para que fosse possível realizar automaticamente os procedimentos de \textit{backup} e \textit{vacuum} esporadicamente;

	\item Criar um sistema de monitoramento em C++ para o sistema Linux capaz de se comunicar com uma aplicação servidor;
	
	\item Criar uma aplicação servidor em Java que capaz de atender a diversas aplicações cliente simultaneamente;
	
	\item Mensurar o desempenho das aplicações desenvolvidas para que fosse possível o uso em produção;

\end{itemize}


\section{Organização da Proposta} \label{Sec:Organizacao}

No \autoref{cap:funTeorica} encontra-se a definição e os conceitos da arquitetura cliente-servidor, \textit{web services}, protocolo HTTP, padrão ReST, e o que são e para que servem os testes automatizados. Esses conceitos são importantes para o entendimento das tecnologias utilizadas no \autoref{cap:metodologia}, que descreve as as ferramentas e os \textit{frameworks} utilizados na fase de implementação.

No \autoref{cap:Desenvolvimento} explica-se o processo de implementação do sistema de monitoramento Linux (\autoref{sec:MonitorWeb-Cli}) e a aplicação servidor, responsável por receber os dados de monitoramento e persisti-las no banco de dados (\autoref{sec:MonitorWeb-Api}). No \autoref{cap:Resultados} são apresentados os resultados dos testes realizados.

Por fim, no \autoref{cap:conclusao} são expostas as considerações finais deste trabalho. Essa análise final é baseada na taxa de consumo de CPU, memória, e \textit{swap} das aplicações cliente e servidor, e no desempenho que a aplicação servidor (\textit{web service}) obteve ao processar uma carga excessiva de requisições por segundo.
